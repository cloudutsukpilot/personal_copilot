âœ… Project Goals

 - Web UI with a Start button
 - Stream audio input from the mic (or other source)
 - Continuously transcribe using OpenAI Whisper (local model)
 - Show transcription output live


ğŸ“ Folder Structure

```php
whisper_transcriber/
â”œâ”€â”€ app.py                  # Main backend app (Flask)
â”œâ”€â”€ transcriber.py          # Whisper logic (transcription pipeline)
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html          # Web UI
â”œâ”€â”€ static/
â”‚   â””â”€â”€ style.css           # Optional styles
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ README.md               # Instructions (optional)
```

ğŸš€ To Run the App

```bash
cd whisper_transcriber
pip install -r requirements.txt
python app.py
```

Open browser: http://127.0.0.1:5000


Improvements:
----------------------
ğŸ” Make it continuous without pressing the button repeatedly?

ğŸ’¬ Add speaker diarization or timestamping?

ğŸª„ Add microphone selector or upload audio file option?


Installation for GPU Support:

1. Install Whisper
- pip install git+https://github.com/openai/whisper.git

2. Install Pytorch
- Check for the version compatibilty - https://pytorch.org/get-started/locally/
- pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

3. Download CUDA Toolkit for Windows
https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_local

4. Test if GPU is used

```python
import torch
print("CUDA available:", torch.cuda.is_available())
print("GPU device:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU")
```



### Containerization

1. Create a docker file

```dockerfile
FROM pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime

RUN pip install git+https://github.com/openai/whisper.git
RUN apt-get update && apt-get install -y ffmpeg
COPY . /app
WORKDIR /app

CMD ["python", "app.py"]
```

2. Enable GPU in Docker

```sh
docker run --gpus all -p 5000:5000 your-image-name
```

- Inside the container, check if CUDA is available:

```sh
python -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0))"
```