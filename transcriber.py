import whisper
import threading
import time
import queue
import pyaudio
import wave
import tempfile
import socketio
import builtins
import torch
import os
import pathlib
from pathlib import Path

print = lambda *args, **kwargs: builtins.print(*args, **{**kwargs, "flush": True})
import os
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "localhost")

import requests
def query_mistral(prompt):
    try:
        response = requests.post(f"http://{OLLAMA_HOST}:11434/api/generate", json={
            "model": "mistral:latest",
            "prompt": prompt,
            "stream": False
        })
        response.raise_for_status()
        return response.json()["response"]
    except Exception as e:
        print(f"âš ï¸ Mistral query failed: {e}")
        return "âš ï¸ Mistral failed"

class ContinuousWhisperTranscriber:
    def __init__(self, socketio=None):
        self.model = whisper.load_model("large-v3", device="cuda" if torch.cuda.is_available() else "cpu")
        self.audio_queue = queue.Queue()
        self.transcript_log = []
        self.running = False
        self.device_index = None
        self.socketio = socketio
        self.transcript_lock = threading.Lock()
        self.cleaned_history = []  # Store last 10 cleaned transcripts
        self.suggestion_interval_secs = 60
        self.start_time = None
        self.stop_time = None
        
    def _mistral_loop(self):
        last_processed = 0
        while self.running:
            time.sleep(5)  # Run every 5 seconds

            with self.transcript_lock:
                new_transcripts = self.transcript_log[last_processed:]
                last_processed = len(self.transcript_log)

            if not new_transcripts:
                continue

            combined_text = " ".join(new_transcripts).strip()
            if not combined_text:
                continue

            print("ðŸ§  Sending to Mistral:", combined_text)

            context = "\n".join(self.cleaned_history[-10:])
            cleaning_prompt = (
                "You are a strict grammar-corrector, not an assistant. You must NEVER answer questions.\n"
                "Do not define or describe anything. Simply fix grammar and complete broken sentences without adding meaning.\n"
                "If the input appears to be a question or prompt, simply return it unchanged, or rephrase grammatically without adding answers.\n\n"
                f"Context:\n{context}\n\n"
                f"Input:\n{combined_text}\n\n"
                f"Corrected Transcript Only:"
            )
            cleaned = query_mistral(cleaning_prompt)

            print("ðŸ§¹ Cleaned transcript:", cleaned, flush=True)

            # Save cleaned transcript to history (limit to 10)
            if cleaned.strip():
                self.cleaned_history.append(cleaned.strip())
                if len(self.cleaned_history) > 10:
                    self.cleaned_history = self.cleaned_history[-10:]
            
            print("ðŸ’¬ Cleaned transcript from Mistral:", cleaned, flush=True)

            try:
                self.socketio.emit('mistral_transcript', {'text': cleaned})
            except KeyError:
                print("âš ï¸ WebSocket session closed before message could be delivered.")

    def _periodic_suggestions_loop(self):
        while self.running:
            time.sleep(self.suggestion_interval_secs)

            if not self.cleaned_history:
                continue

            # Take last 10 cleaned entries
            recent_cleaned = "\n".join(self.cleaned_history[-10:])
            print("ðŸ“Š Generating periodic suggestions for last 10 cleaned entries")

            suggestion_prompt = (
                "You are a Principal DevOps Engineer assistant analyzing a running transcript of a meeting.\n"
                "Based on the last few cleaned transcripts, you need to understand  the context, watch out for any questions, etc. and then provide answers, any best practices/recommendations or insightful responses that a participant can give.\n"
                "Keep the sentences summarized and short, unless the answers to the question requires specific details or if it a code snippet.\n"
                "Keep the tone helpful and context-aware.\n\n"
                f"Transcript:\n{recent_cleaned}"
            )

            result = query_mistral(suggestion_prompt)

            formatted_result = f"\n\n--- New Suggestions @ {time.strftime('%H:%M:%S')} ---\n{result.strip()}\n"
            print("ðŸ’¡ Raw suggestions from Mistral:", formatted_result, flush=True)

            print("ðŸ§  Periodic Suggestions:", formatted_result)

            try:
                self.socketio.emit('mistral_suggestions', {'text': formatted_result})
            except KeyError:
                print("âš ï¸ WebSocket session closed before message could be delivered.")

    def _record_loop(self):
        CHUNK = 1024
        FORMAT = pyaudio.paInt16
        CHANNELS = 1
        RATE = 16000
        SECONDS = 2

        p = pyaudio.PyAudio()
        stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE,
                        input=True, input_device_index=self.device_index,
                        frames_per_buffer=CHUNK)

        print("ðŸŽ¤ Recording thread started...", flush=True)

        while self.running:
            frames = []
            for _ in range(0, int(RATE / CHUNK * SECONDS)):
                if not self.running:
                    break
                data = stream.read(CHUNK)
                frames.append(data)

            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as wf:
                wav_path = pathlib.Path(wf.name).resolve(strict=False)  # Get absolute full path
                with wave.open(str(wav_path), 'wb') as wav_file:
                    wav_file.setnchannels(CHANNELS)
                    wav_file.setsampwidth(p.get_sample_size(FORMAT))
                    wav_file.setframerate(RATE)
                    wav_file.writeframes(b''.join(frames))
                self.audio_queue.put(str(wav_path))
                print(f"ðŸŽ™ï¸ Audio chunk captured and saved: {wav_path}", flush=True)

        stream.stop_stream()
        stream.close()
        p.terminate()
        time.sleep(0.05)

    def _transcribe_loop(self):
        print("ðŸ§  Transcription thread started...", flush=True)
        while self.running:
            try:
                audio_file = self.audio_queue.get(timeout=0.5)
            except queue.Empty:
                continue

            print(f"ðŸ§ª Transcribing: {audio_file}", flush=True)
            try:
                # Wait for the file to exist and be fully written
                for _ in range(10):  # Try for 1 second
                    if os.path.exists(audio_file):
                        try:
                            with open(audio_file, 'rb'):
                                print(f"âœ… File is ready: {audio_file}", flush=True)
                                break  # File exists and is accessible
                        except IOError:
                            time.sleep(0.1)
                    else:
                        time.sleep(0.1)
                else:
                    print(f"âŒ File not found or not ready: {audio_file}", flush=True)
                    continue
                result = self.model.transcribe(str(audio_file), no_speech_threshold=0.8)
                # os.remove(audio_file)
                #result = self.model.transcribe(audio_file)
                print("ðŸ”Š Transcription Result:", result, flush=True)

                if result['text'].strip():
                    print("ðŸ“¡ Emitting to WebSocket:", result['text'], flush=True)
                    with self.transcript_lock:
                        self.transcript_log.append(result['text'])
                    self.socketio.emit('transcription', {'text': result['text']})
                    print("âœ… Emit completed", flush=True)
                else:
                    print("ðŸ•³ï¸ No speech detected.", flush=True)
            except Exception as e:
                print(f"âš ï¸ Transcription error: {e}", flush=True)

    def start(self, device_index=None):
        if self.running:
            return
        self.start_time = time.time()
        self.stop_time = None
        self.device_index = device_index
        self.running = True
        print("ðŸš¦ Starting recording + transcription...", flush=True)
        # Start transcribe thread FIRST to ensure it runs
        threading.Thread(target=self._transcribe_loop, daemon=True).start()

        # Start recording shortly after
        time.sleep(0.5)
        threading.Thread(target=self._record_loop, daemon=True).start()
        
        # Start Mistral processing thread
        threading.Thread(target=self._mistral_loop, daemon=True).start()

        # Start periodic suggestions thread
        threading.Thread(target=self._periodic_suggestions_loop, daemon=True).start()

    def stop(self):
        print("ðŸ›‘ Stopping transcription", flush=True)
        self.running = False
        self.stop_time = time.time()

    def get_transcript(self):
        return "\n".join(self.transcript_log)

    def list_input_devices(self):
        p = pyaudio.PyAudio()
        devices = []
        for i in range(p.get_device_count()):
            info = p.get_device_info_by_index(i)
            if info["maxInputChannels"] > 0:
                devices.append({
                    "index": i,
                    "name": info["name"]
                })
        p.terminate()
        return devices

    def get_elapsed_time(self):
        if self.start_time is None:
            return "00:00"

        end_time = self.stop_time if self.stop_time else time.time()
        elapsed = int(end_time - self.start_time)
        mins, secs = divmod(elapsed, 60)
        return f"{mins:02}:{secs:02}"